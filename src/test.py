import dxtb
import torch

print("=== Implementing Gradient Flow Solution ===")

# Step 1: Create optimizable parameter tensors
h_params = dxtb.GFN1_XTB.element['H']
print(f"Original H gamma: {h_params.gam}")
# Create tensor that we want to optimize
gam_tensor = torch.tensor(h_params.gam, requires_grad=True, dtype=torch.float32)

class DXTBEnergyFunction(torch.autograd.Function):
    @staticmethod
    def forward(ctx, gam_param, numbers, positions):
        ctx.save_for_backward(gam_param, numbers, positions)
        ctx.original_gam = dxtb.GFN1_XTB.element['H'].gam
        
        dxtb.GFN1_XTB.element['H'].gam = gam_param.item()
        
        try:
            calc = dxtb.calculators.GFN1Calculator(numbers, dtype=torch.float32)
            result = calc.singlepoint(positions)
            energy = result.total.item() if result.total.numel() == 1 else result.total.sum().item()
            return torch.tensor(energy, dtype=torch.float32)
        finally:
            dxtb.GFN1_XTB.element['H'].gam = ctx.original_gam
    
    @staticmethod
    def backward(ctx, grad_output):
        gam_param, numbers, positions = ctx.saved_tensors
        original_gam = ctx.original_gam
        eps = 1e-6
        
        def compute_energy(gam_value):
            dxtb.GFN1_XTB.element['H'].gam = gam_value
            calc = dxtb.calculators.GFN1Calculator(numbers, dtype=torch.float32)
            result = calc.singlepoint(positions)
            return result.total.item() if result.total.numel() == 1 else result.total.sum().item()
        
        try:
            energy_plus = compute_energy(gam_param.item() + eps)
            energy_minus = compute_energy(gam_param.item() - eps)
            grad_gam = (energy_plus - energy_minus) / (2 * eps)
            return grad_output * grad_gam, None, None
        finally:
            dxtb.GFN1_XTB.element['H'].gam = original_gam

# Step 3: Test the custom autograd function with validation
print("=== Testing Custom Autograd Function ===")

def test_gradient_flow():
    """Test gradient flow with comprehensive validation."""
    numbers = torch.tensor([1, 1], dtype=torch.long)
    positions = torch.tensor([[0.0, 0.0, 0.0], [0.0, 0.0, 1.0]], dtype=torch.float32)
    
    # Reset parameter to original value
    gam_tensor.data = torch.tensor(h_params.gam, dtype=torch.float32)
    if gam_tensor.grad is not None:
        gam_tensor.grad.zero_()
    
    print(f"Initial parameter: {gam_tensor.item():.8f}")
    
    # Test 1: Energy computation
    energy = DXTBEnergyFunction.apply(gam_tensor, numbers, positions)
    print(f"âœ“ Energy: {energy.item():.8f} Hartree")
    assert energy.requires_grad, "Energy must have gradients!"
    
    # Test 2: Gradient computation (may be small for gamma parameter)
    print("Computing gradients...")
    energy = DXTBEnergyFunction.apply(gam_tensor, numbers, positions)
    loss = energy ** 2
    loss.backward()
    
    print(f"âœ“ Parameter: {gam_tensor.item():.8f}")
    print(f"âœ“ Energy: {energy.item():.8f} Hartree")
    print(f"âœ“ Gradient: {gam_tensor.grad.item():.2e}")
    
    # Test 3: Gradient computation works (value may be small due to physics)
    assert gam_tensor.grad is not None, "Gradient must be computed!"
    print("âœ“ Gradient computation successful (small values are expected for gamma)")
    
    # Test 4: Parameter restoration check
    original_after = dxtb.GFN1_XTB.element['H'].gam
    expected_original = h_params.gam
    assert abs(original_after - expected_original) < 1e-10, f"Parameter not restored! {original_after} != {expected_original}"
    
    print("ðŸŽ‰ All tests passed!")
    return True

try:
    test_gradient_flow()
except Exception as e:
    print(f"âŒ Test failed: {e}")
    import traceback
    traceback.print_exc()

# Step 4: Final validation - H2 energy curve
print(f"\n=== H2 Energy Curve Validation ===")

def h2_energy_with_gradients(gam_tensor, distance=1.0):
    """Compute H2 energy at given distance with gradient support."""
    numbers = torch.tensor([1, 1], dtype=torch.long)
    positions = torch.tensor([[0.0, 0.0, 0.0], [0.0, 0.0, distance]], dtype=torch.float32)
    return DXTBEnergyFunction.apply(gam_tensor, numbers, positions)

# Reset parameter
gam_tensor.data = torch.tensor(h_params.gam, dtype=torch.float32)

print("H2 energy curve with original parameters:")
distances = [0.5, 0.74, 1.0, 1.5]
for dist in distances:
    energy = h2_energy_with_gradients(gam_tensor, distance=dist)
    print(f"  {dist:.2f} Ã…: {energy.item():.6f} Hartree")


